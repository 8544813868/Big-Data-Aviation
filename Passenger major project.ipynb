{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90717f6-41dc-468e-8476-dadf7fe3f521",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------------------------------------+\n|FirstName|LastName   |MoreDetails                             |\n+---------+-----------+----------------------------------------+\n|Phil     |Hawkins    |{R-68106487, {M, Veg, MTk4NS0wNy0xNw==}}|\n|Jay      |Singh      |{P-64549526, {F, Any, MTk5NS0wMi0wOA==}}|\n|Russell  |Constantine|{H-44102824, {M, Any, MjAwMS0wNS0yMw==}}|\n|Phil     |Singh      |{D-61495909, {M, Veg, MTk4NS0wOS0xMQ==}}|\n|Robert   |Banner     |{U-49850866, {M, Any, MTk5MS0wNC0yNA==}}|\n|Heather  |Singh      |{S-26384273, {M, Any, MTk4Ny0xMS0xOA==}}|\n|Billy    |Kohli      |{J-42970426, {M, Veg, MTk5MS0wMS0zMA==}}|\n|Mary     |Singh      |{A-69169562, {F, Veg, MTk5Ny0wMy0wNQ==}}|\n+---------+-----------+----------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import*\n",
    "from pyspark.sql.functions import*\n",
    "data = [\n",
    "    {\"FirstName\": \"Phil\", \"LastName\": \"Hawkins\", \"MoreDetails\": {\"Passport\": \"R-68106487\", \"PersonalDetails\": {\"Gender\": \"M\", \"MealPreference\": \"Veg\", \"DOB\": \"MTk4NS0wNy0xNw==\"}}},\n",
    "    {\"FirstName\": \"Jay\", \"LastName\": \"Singh\", \"MoreDetails\": {\"Passport\": \"P-64549526\", \"PersonalDetails\": {\"Gender\": \"F\", \"MealPreference\": \"Any\", \"DOB\": \"MTk5NS0wMi0wOA==\"}}},\n",
    "    {\"FirstName\": \"Russell\", \"LastName\": \"Constantine\", \"MoreDetails\": {\"Passport\": \"H-44102824\", \"PersonalDetails\": {\"Gender\": \"M\", \"MealPreference\": \"Any\", \"DOB\": \"MjAwMS0wNS0yMw==\"}}},\n",
    "    {\"FirstName\": \"Phil\", \"LastName\": \"Singh\", \"MoreDetails\": {\"Passport\": \"D-61495909\", \"PersonalDetails\": {\"Gender\": \"M\", \"MealPreference\": \"Veg\", \"DOB\": \"MTk4NS0wOS0xMQ==\"}}},\n",
    "    {\"FirstName\": \"Robert\", \"LastName\": \"Banner\", \"MoreDetails\": {\"Passport\": \"U-49850866\", \"PersonalDetails\": {\"Gender\": \"M\", \"MealPreference\": \"Any\", \"DOB\": \"MTk5MS0wNC0yNA==\"}}},\n",
    "    {\"FirstName\": \"Heather\", \"LastName\": \"Singh\", \"MoreDetails\": {\"Passport\": \"S-26384273\", \"PersonalDetails\": {\"Gender\": \"M\", \"MealPreference\": \"Any\", \"DOB\": \"MTk4Ny0xMS0xOA==\"}}},\n",
    "    {\"FirstName\": \"Billy\", \"LastName\": \"Kohli\", \"MoreDetails\": {\"Passport\": \"J-42970426\", \"PersonalDetails\": {\"Gender\": \"M\", \"MealPreference\": \"Veg\", \"DOB\": \"MTk5MS0wMS0zMA==\"}}},\n",
    "    {\"FirstName\": \"Mary\", \"LastName\": \"Singh\", \"MoreDetails\": {\"Passport\": \"A-69169562\", \"PersonalDetails\": {\"Gender\": \"F\", \"MealPreference\": \"Veg\", \"DOB\": \"MTk5Ny0wMy0wNQ==\"}}}\n",
    "]\n",
    "schema = StructType([\n",
    "    StructField(\"FirstName\", StringType(), True),\n",
    "    StructField(\"LastName\", StringType(), True),\n",
    "    StructField(\"MoreDetails\", StructType([\n",
    "        StructField(\"Passport\", StringType(), True),\n",
    "        StructField(\"PersonalDetails\", StructType([\n",
    "            StructField(\"Gender\", StringType(), True),\n",
    "            StructField(\"MealPreference\", StringType(), True),\n",
    "            StructField(\"DOB\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b8a8f5-55e4-408a-820f-88ce7a9451b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+------+--------------+----------------+\n|FirstName|LastName   |Passport  |Gender|MealPreference|DOB             |\n+---------+-----------+----------+------+--------------+----------------+\n|Phil     |Hawkins    |R-68106487|M     |Veg           |MTk4NS0wNy0xNw==|\n|Jay      |Singh      |P-64549526|F     |Any           |MTk5NS0wMi0wOA==|\n|Russell  |Constantine|H-44102824|M     |Any           |MjAwMS0wNS0yMw==|\n|Phil     |Singh      |D-61495909|M     |Veg           |MTk4NS0wOS0xMQ==|\n|Robert   |Banner     |U-49850866|M     |Any           |MTk5MS0wNC0yNA==|\n|Heather  |Singh      |S-26384273|M     |Any           |MTk4Ny0xMS0xOA==|\n|Billy    |Kohli      |J-42970426|M     |Veg           |MTk5MS0wMS0zMA==|\n|Mary     |Singh      |A-69169562|F     |Veg           |MTk5Ny0wMy0wNQ==|\n+---------+-----------+----------+------+--------------+----------------+\n\nroot\n |-- FirstName: string (nullable = true)\n |-- LastName: string (nullable = true)\n |-- Passport: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- MealPreference: string (nullable = true)\n |-- DOB: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import*\n",
    "from pyspark.sql.functions import*\n",
    "df_flat = df.select(\n",
    "    col(\"FirstName\"),\n",
    "    col(\"LastName\"),\n",
    "    col(\"MoreDetails.Passport\").alias(\"Passport\"),\n",
    "    col(\"MoreDetails.PersonalDetails.Gender\").alias(\"Gender\"),\n",
    "    col(\"MoreDetails.PersonalDetails.MealPreference\").alias(\"MealPreference\"),\n",
    "    col(\"MoreDetails.PersonalDetails.DOB\").alias(\"DOB\")\n",
    ")\n",
    "\n",
    "# Show the flattened DataFrame\n",
    "df_flat.show(truncate=False)\n",
    "\n",
    "# Print the schema of the flattened DataFrame\n",
    "df_flat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86188b3-2231-440c-8aa6-60f039edaf31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download URL: https://community.cloud.databricks.com/files/tables/passenger_details.xlsx\n"
     ]
    }
   ],
   "source": [
    "# converts the PySpark DataFrame into a Pandas DataFrame\n",
    "result_pd = df_flat.toPandas()\n",
    "\n",
    "# Define the local path to save the excel file which means the file will be saved in the /tmp directory with the name passenger_details.xlsx.\n",
    "local_path = \"/tmp/passenger_details.xlsx\"\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "result_pd.to_excel(local_path, index=False)\n",
    "\n",
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/passenger_details.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n",
    "\n",
    "# Print the URL to download the file\n",
    "print(f\"Download URL: https://community.cloud.databricks.com/files/tables/passenger_details.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7ed0377-2765-4751-ab1a-b64b639a1dfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "transform layer - Decrypt DOB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb306036-a8a6-41f0-ab46-2d94802c9edb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting openpyxl\n  Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\nCollecting et-xmlfile\n  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.1.5\nPython interpreter will be restarted.\n+---------+-----------+----------+------+--------------+----------+\n|FirstName|LastName   |Passport  |Gender|MealPreference|DOB       |\n+---------+-----------+----------+------+--------------+----------+\n|Phil     |Hawkins    |R-68106487|M     |Veg           |1985-07-17|\n|Jay      |Singh      |P-64549526|F     |Any           |1995-02-08|\n|Russell  |Constantine|H-44102824|M     |Any           |2001-05-23|\n|Phil     |Singh      |D-61495909|M     |Veg           |1985-09-11|\n|Robert   |Banner     |U-49850866|M     |Any           |1991-04-24|\n|Heather  |Singh      |S-26384273|M     |Any           |1987-11-18|\n|Billy    |Kohli      |J-42970426|M     |Veg           |1991-01-30|\n|Mary     |Singh      |A-69169562|F     |Veg           |1997-03-05|\n+---------+-----------+----------+------+--------------+----------+\n\nDownload URL: https://community.cloud.databricks.com/files/tables/decrypted_dob.xlsx\n"
     ]
    }
   ],
   "source": [
    "# decode_base64(encoded_str): A function that takes a base64 encoded string as input, decodes it, and returns the decoded string.\n",
    "# base64.b64decode(encoded_str): Decodes the base64 encoded string to bytes.\n",
    "# decoded_bytes.decode('utf-8'): Converts the decoded bytes to a UTF-8 string.\n",
    "%pip install openpyxl\n",
    "\n",
    "import base64\n",
    "import base64\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def decode_base64(encoded_str):\n",
    "    try:\n",
    "        decoded_bytes = base64.b64decode(encoded_str)   \n",
    "        decoded_str = decoded_bytes.decode('utf-8')\n",
    "        return decoded_str\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Convert the function to a PySpark UDF\n",
    "decode_base64_udf = udf(decode_base64, StringType())\n",
    "\n",
    "# Apply the UDF to decrypt the DOB column\n",
    "df_flat_decrypted = df_flat.withColumn(\"DOB\", decode_base64_udf(col(\"DOB\")))\n",
    "\n",
    "# Show the DataFrame with decrypted DOB\n",
    "df_flat_decrypted.show(truncate=False)\n",
    "\n",
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "result_pd_df = df_flat_decrypted.toPandas()\n",
    "\n",
    "# Define the local path to save the file\n",
    "local_path = \"/tmp/decrypted_dob.xlsx\"\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "result_pd_df.to_excel(local_path, index=False)\n",
    "\n",
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/decrypted_dob.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n",
    "\n",
    "# Provide the URL to download the file\n",
    "url = \"https://community.cloud.databricks.com/files/tables/decrypted_dob.xlsx\"\n",
    "print(\"Download URL:\", url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2c3f3ec-1f57-434b-88db-4e0b3732119f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "agg layer - Find the passengers those were travelling on his/her birthday for by travel months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7614ea09-8b1e-4266-801f-f4e4aa0864aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+---------+------------+----------+-----------+\n|FirstName|LastName|Revenue|Origin   |Destination |DOB       |TravelMonth|\n+---------+--------+-------+---------+------------+----------+-----------+\n|Phil     |Hawkins |67.73  |Atlanta  |Phoenix     |1985-07-17|7          |\n|Jay      |Singh   |349.07 |Dallas   |Charlotte   |1995-02-08|2          |\n|Heather  |Singh   |384.76 |Las Vegas|Phoenix     |1987-11-18|11         |\n|Billy    |Kohli   |148.16 |Dallas   |Philadelphia|1991-01-30|1          |\n+---------+--------+-------+---------+------------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, month, dayofmonth, to_date\n",
    "\n",
    "# Load trips data\n",
    "df_trips = spark.read.csv(\"/FileStore/tables/travelmonth.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert TravelDate to Date format basically string format to date format     \n",
    "df_trips = df_trips.withColumn(\"TravelDate\", to_date(col(\"TravelDate\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Rename columns in df_trips to avoid ambiguity\n",
    "df_trips_renamed = df_trips.withColumnRenamed(\"FirstName\", \"TripFirstName\") \\\n",
    "                           .withColumnRenamed(\"LastName\", \"TripLastName\") \\\n",
    "                           .withColumnRenamed(\"Revenue\", \"TripRevenue\") \\\n",
    "                           .withColumnRenamed(\"Origin\", \"TripOrigin\") \\\n",
    "                           .withColumnRenamed(\"Destination\", \"TripDestination\")\n",
    "\n",
    "# Join DataFrames on Passport\n",
    "df_combined = df_passengers_with_dob.join(df_trips_renamed, on=\"Passport\", how=\"inner\")\n",
    "\n",
    "# Add columns for travel month and birth month/day\n",
    "df_combined = df_combined.withColumn(\"TravelMonth\", month(col(\"TravelDate\"))) \\\n",
    "                         .withColumn(\"BirthMonth\", month(col(\"DOB\"))) \\\n",
    "                         .withColumn(\"BirthDay\", dayofmonth(col(\"DOB\")))\n",
    "\n",
    "# Filter passengers traveling on their birthday\n",
    "df_birthday_travelers = df_combined.filter(\n",
    "    (col(\"TravelMonth\") == col(\"BirthMonth\")) &\n",
    "    (col(\"BirthDay\") == dayofmonth(col(\"TravelDate\")))\n",
    ")\n",
    "\n",
    "# Select required columns\n",
    "df_result = df_birthday_travelers.select(\n",
    "    col(\"FirstName\"),\n",
    "    col(\"LastName\"),\n",
    "    col(\"TripRevenue\").alias(\"Revenue\"),\n",
    "    col(\"TripOrigin\").alias(\"Origin\"),\n",
    "    col(\"TripDestination\").alias(\"Destination\"),\n",
    "    col(\"DOB\"),\n",
    "    col(\"TravelMonth\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9691b9f-4cde-47dc-a63d-063ab02ef5c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download URL: https://community.cloud.databricks.com/files/tables/Birthday_Travelers.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the result DataFrame to a Pandas DataFrame\n",
    "result_pd = df_result.toPandas()\n",
    "\n",
    "# Define the local path to save the file\n",
    "local_path = \"/tmp/Birthday_Travelers.xlsx\"\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "result_pd.to_excel(local_path, index=False)\n",
    "\n",
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/Birthday_Travelers.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n",
    "\n",
    "# Print the URL to download the file\n",
    "print(f\"Download URL: https://community.cloud.databricks.com/files/tables/Birthday_Travelers.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0179deb-fa7f-4ad5-83d9-ddf091bc8260",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "agg layer - What is the most common meal preference for travel month Jan-2021?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad20e25-1410-4388-9e3e-a6c7afe90e18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n|MealPreference|count|\n+--------------+-----+\n|           Veg|  143|\n+--------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter for travel month January 2021\n",
    "df_january_2021 = df_combined.filter((col(\"TravelMonth\") == 1) & (year(col(\"TravelDate\")) == 2021))\n",
    "\n",
    "# Group by MealPreference and count occurrences\n",
    "meal_preference_counts = df_january_2021.groupBy(\"MealPreference\").count()\n",
    "\n",
    "# Find the most common meal preference\n",
    "most_common_meal_preference = meal_preference_counts.orderBy(col(\"count\").desc()).limit(1)\n",
    "\n",
    "# Show the result\n",
    "most_common_meal_preference.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901238dc-5db1-4cab-bec8-d6d2ad508d47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download URL: https://community.cloud.databricks.com/files/tables/Most_Common_Meal_Preference_Jan2021.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "# Filter for travel month January 2021\n",
    "df_january_2021 = df_combined.filter((col(\"TravelMonth\") == 1) & (year(col(\"TravelDate\")) == 2021))\n",
    "\n",
    "# Group by MealPreference and count occurrences\n",
    "meal_preference_counts = df_january_2021.groupBy(\"MealPreference\").count()\n",
    "\n",
    "# Find the most common meal preference\n",
    "most_common_meal_preference = meal_preference_counts.orderBy(col(\"count\").desc()).limit(1)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "most_common_meal_preference_pd = most_common_meal_preference.toPandas()\n",
    "\n",
    "# Define the local path to save the file\n",
    "local_path = \"/tmp/Most_Common_Meal_Preference_Jan2021.xlsx\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "most_common_meal_preference_pd.to_excel(local_path, index=False)\n",
    "\n",
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/Most_Common_Meal_Preference_Jan2021.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n",
    "\n",
    "# Print the URL to download the file\n",
    "print(f\"Download URL: https://community.cloud.databricks.com/files/tables/Most_Common_Meal_Preference_Jan2021.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae24216f-c283-4e4d-9182-e574c1675301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Passenger major project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
