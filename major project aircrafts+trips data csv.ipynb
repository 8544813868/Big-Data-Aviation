{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c1cc9e-90bb-4de0-963f-6141f9073c86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (1.4.2)\nCollecting openpyxl\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\nCollecting xlsxwriter\n  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.9/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas) (2021.3)\nRequirement already satisfied: numpy>=1.18.5 in /databricks/python3/lib/python3.9/site-packages (from pandas) (1.21.5)\nCollecting et-xmlfile\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nInstalling collected packages: et-xmlfile, xlsxwriter, openpyxl\nSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.1.5 xlsxwriter-3.2.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl xlsxwriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11c85f24-c731-4b12-87f0-8f015634e341",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "aircrafts transform layer - Add a new attribute “Make” & retain all existing attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b089d64-d130-403d-91b5-a0706d92b478",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+------+\n|AircraftName|Capacity|TailNumber|  Make|\n+------------+--------+----------+------+\n| Airbus-A330|     131|     T2366|Airbus|\n|  Boeing-747|     213|     O2349|Boeing|\n| Airbus-A330|      42|     C2888|Airbus|\n|  Boeing-747|     140|     C2896|Boeing|\n|  Boeing-777|     191|     F2838|Boeing|\n| Airbus-A340|     125|     Y2161|Airbus|\n| Airbus-A330|     235|     C2990|Airbus|\n|  Boeing-777|     168|     P2752|Boeing|\n|  Boeing-787|     240|     T2335|Boeing|\n| Airbus-A350|      94|     C2386|Airbus|\n| Airbus-A300|      73|     O2378|Airbus|\n| Airbus-A340|      48|     Z2357|Airbus|\n|  Boeing-767|     209|     J2465|Boeing|\n|  Boeing-747|     249|     R2852|Boeing|\n| Airbus-A350|      24|     N2252|Airbus|\n|  Boeing-787|     168|     H3059|Boeing|\n| Airbus-A350|      96|     A2975|Airbus|\n|  Boeing-777|     226|     C2750|Boeing|\n|  Boeing-747|      29|     S2869|Boeing|\n|  Boeing-767|     164|     G2971|Boeing|\n+------------+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "\n",
    "# Load the aircrafts CSV file\n",
    "aircrafts_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/tables/aircrafts-1.csv\")\n",
    "\n",
    "# Add the \"Make\" column based on the \"AircraftName\" column in the aircrafts DataFrame\n",
    "aircrafts_df_with_make = aircrafts_df.withColumn(\n",
    "    \"Make\",\n",
    "    when(col(\"AircraftName\").contains(\"Airbus\"), \"Airbus\")\n",
    "    .when(col(\"AircraftName\").contains(\"Boeing\"), \"Boeing\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# Display the updated aircrafts DataFrame with the \"Make\" column\n",
    "aircrafts_df_with_make.show()\n",
    "\n",
    "# Optionally, save the updated aircrafts DataFrame with the Make column\n",
    "aircrafts_df_with_make.write.mode(\"overwrite\").parquet(\"/path/to/save/aircrafts_with_make_final.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b208f44-006f-43dd-b65e-965cb40d9e3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 20\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of rows in the resulting DataFrame\n",
    "total_rows = aircrafts_df_with_make.count()\n",
    "\n",
    "# Print the total number of rows\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d378624f-4124-4cc7-b3b8-d3853e8ae5ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "TRIPS DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ee9b721-06c6-4bb7-8666-d4ce56e7ec87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "transorm layer trips -  Origin & Destination should be transformed to city name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb69545d-da5a-4b42-b96a-1bd096121cf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+----------+---------------+-------+---------+-----------+----------+------------+----------+\n|tailnumber|     origin|    destination|traveldate|totalflighttime|revenue|firstname|   lastname|  passport|aircraftname|totalmonth|\n+----------+-----------+---------------+----------+---------------+-------+---------+-----------+----------+------------+----------+\n|     C2888|   New York|Fort Lauderdale|2021-04-07|              9| 384.93|      Jay|      Singh|P-64549526|      Airbus|   04-2021|\n|     H3059|     Denver|        Houston|2021-08-17|              2| 216.11|  Russell|Constantine|H-44102824|      Boeing|   08-2021|\n|     J2465|    Seattle|        Detroit|2021-02-03|              6| 195.73|      Jay|      Singh|P-64549526|      Boeing|   02-2021|\n|     A2975|    Orlando|Fort Lauderdale|2021-08-12|              1| 351.65|     Phil|    Hawkins|R-68106487|      Airbus|   08-2021|\n|     C2386|    Seattle|Fort Lauderdale|2021-06-29|              2| 211.72|  Russell|Constantine|H-44102824|      Airbus|   06-2021|\n|     G2971|    Atlanta|          Miami|2020-11-23|              9| 166.93|     Phil|    Hawkins|R-68106487|      Boeing|   11-2020|\n|     O2378|    Seattle|         Newark|2021-09-09|              1| 310.64|     Phil|      Singh|D-61495909|      Airbus|   09-2021|\n|     C2386|     Dallas|          Miami|2020-12-17|              5| 454.54|     Mary|      Singh|A-69169562|      Airbus|   12-2020|\n|     Y2161|     Dallas|        Phoenix|2021-04-05|              5| 111.92|    Billy|      Kohli|J-42970426|      Airbus|   04-2021|\n|     C2750|   New York|        Phoenix|2021-06-02|              5| 400.23|    Billy|      Kohli|J-42970426|      Boeing|   06-2021|\n|     P2752|    Atlanta|Fort Lauderdale|2021-04-20|              8| 208.62|  Heather|      Singh|S-26384273|      Boeing|   04-2021|\n|     T2366|  Las Vegas|Fort Lauderdale|2021-01-13|              5| 318.32|     Phil|    Hawkins|R-68106487|      Airbus|   01-2021|\n|     T2335|    Chicago|         Boston|2021-08-14|              4| 121.53|     Phil|      Singh|D-61495909|      Boeing|   08-2021|\n|     C2896|  Las Vegas|          Miami|2021-03-11|              4| 274.73|      Jay|      Singh|P-64549526|      Boeing|   03-2021|\n|     C2888|    Atlanta|          Miami|2021-02-27|              4| 239.21|      Jay|      Singh|P-64549526|      Airbus|   02-2021|\n|     T2366|     Dallas|         Boston|2021-10-05|              4| 381.61|  Russell|Constantine|H-44102824|      Airbus|   10-2021|\n|     C2896|   New York|          Miami|2020-12-24|              8|   56.0|    Billy|      Kohli|J-42970426|      Boeing|   12-2020|\n|     P2752|Los Angeles|        Detroit|2021-04-19|              7|  408.5|     Mary|      Singh|A-69169562|      Boeing|   04-2021|\n|     A2975|Los Angeles|    Minneapolis|2020-10-29|             10| 415.01|     Phil|    Hawkins|R-68106487|      Airbus|   10-2020|\n|     A2975|  Las Vegas|Fort Lauderdale|2021-03-26|             10| 327.62|  Russell|Constantine|H-44102824|      Airbus|   03-2021|\n+----------+-----------+---------------+----------+---------------+-------+---------+-----------+----------+------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, concat, substring, lit, cast\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Trips Data Processing\").getOrCreate()\n",
    "\n",
    "# Load the trips data\n",
    "df_trips = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/Trips_Data-2.csv\")\n",
    "\n",
    "# Define UDFs for origin and destination mappings\n",
    "origin = {\n",
    "    \"LAS\": \"Las Vegas\", \"DEN\": \"Denver\", \"SEA\": \"Seattle\", \"DFW\": \"Dallas\",\n",
    "    \"SFO\": \"San Francisco\", \"ATL\": \"Atlanta\", \"ORD\": \"Chicago\", \"LAX\": \"Los Angeles\",\n",
    "    \"MCO\": \"Orlando\", \"JFK\": \"New York\"\n",
    "}\n",
    "destination = {\n",
    "    \"IAH\": \"Houston\", \"BOS\": \"Boston\", \"EWR\": \"Newark\", \"CLT\": \"Charlotte\",\n",
    "    \"MIA\": \"Miami\", \"PHX\": \"Phoenix\", \"FLL\": \"Fort Lauderdale\", \"DTW\": \"Detroit\",\n",
    "    \"MSP\": \"Minneapolis\", \"PHL\": \"Philadelphia\"\n",
    "}\n",
    "\n",
    "origin_udf = udf(lambda x: origin.get(x, x), StringType())\n",
    "destination_udf = udf(lambda x: destination.get(x, x), StringType())\n",
    "\n",
    "# Add mapped origin and destination columns\n",
    "df_trips = df_trips.withColumn(\"origin1\", origin_udf(col(\"origin\"))) \\\n",
    "                   .withColumn(\"dest1\", destination_udf(col(\"destination\")))\n",
    "\n",
    "# Load the updated aircrafts data with the Make column\n",
    "df_aircrafts = spark.read.format(\"parquet\").load(\"/path/to/save/aircrafts_with_make_final.parquet\")\n",
    "\n",
    "# Join trips data with aircrafts data\n",
    "df_trips = df_trips.join(df_aircrafts, df_trips[\"tailnumber\"] == df_aircrafts[\"tailnumber\"]) \\\n",
    "                   .select(df_trips[\"*\"], df_aircrafts[\"Make\"].alias(\"aircraftname\"))\n",
    "\n",
    "# Select and cast necessary columns\n",
    "df_trips = df_trips.select(\n",
    "    col(\"tailnumber\"),\n",
    "    col(\"origin1\").alias(\"origin\"),\n",
    "    col(\"dest1\").alias(\"destination\"),\n",
    "    col(\"traveldate\"),\n",
    "    col(\"totalflighttime\").cast(IntegerType()),\n",
    "    col(\"revenue\").cast(DoubleType()),\n",
    "    col(\"firstname\"),\n",
    "    col(\"lastname\"),\n",
    "    col(\"passport\"),\n",
    "    col(\"aircraftname\"),\n",
    "    concat(\n",
    "        substring(col(\"traveldate\"), 6, 2), lit(\"-\"),  # Month\n",
    "        substring(col(\"traveldate\"), 1, 4)             # Year\n",
    "    ).alias(\"totalmonth\")\n",
    ")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_trips.show()\n",
    "\n",
    "# Save the resulting DataFrame as needed\n",
    "df_trips.write.mode(\"overwrite\").parquet(\"/path/to/output/directory/trips_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a4b0946-942e-45f2-8338-e50d0210eccb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 2999\n"
     ]
    }
   ],
   "source": [
    "total_rows = df_trips.count()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef365001-4a94-48c7-85d5-4c05743d4aa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "aggregate layer trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c4ad89-97ff-47e7-a8b6-fa2a1a047496",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "a) Total Revenue by Travel month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6dfa67c-fc91-4a03-ace0-a8cfabd6a88a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n|travel_month|    total_revenue|\n+------------+-----------------+\n|          12|         73236.44|\n|           1|         69188.76|\n|           6|58494.95000000003|\n|           3|65765.44999999998|\n|           5|65789.93000000002|\n|           9|66490.07999999999|\n|           4|61736.66000000001|\n|           8|80308.53999999998|\n|           7|69838.15999999997|\n|          10|67833.41999999994|\n|          11|71063.48999999996|\n|           2|61803.07000000001|\n+------------+-----------------+\n\nDownload URL: https://community.cloud.databricks.com/files/tables/tripsrevenue.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import sum, month\n",
    "\n",
    "# Group by the month extracted from 'totalmonth' and calculate the total revenue for each month\n",
    "total_revenue_by_month = df_trips.groupBy(month(\"totalmonth\").alias(\"travel_month\")).agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
    "\n",
    "# Show the result\n",
    "total_revenue_by_month.show()\n",
    "\n",
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "result_pdtripsrevenue_df = total_revenue_by_month.toPandas()\n",
    "\n",
    "# Define the local path to save the file\n",
    "local_path = \"/tmp/tripsrevenue.xlsx\"\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "result_pdtripsrevenue_df.to_excel(local_path, index=False)\n",
    "\n",
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/tripsrevenue.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n",
    "\n",
    "# Print the URL to download the file\n",
    "print(f\"Download URL: https://community.cloud.databricks.com/files/tables/tripsrevenue.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d3f65b-b03a-479a-b6a8-7a641a0ce777",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "b) Total Revenue by FirstName, LastName."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdad949f-b92e-4626-a41a-3753bc8f35cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------+\n|firstname|   lastname|     total_revenue|\n+---------+-----------+------------------+\n|  Russell|Constantine| 94070.26999999996|\n|    Billy|      Kohli|104907.63000000006|\n|     Mary|      Singh|101128.36000000002|\n|   Robert|     Banner|          98323.88|\n|      Jay|      Singh|105116.43999999996|\n|     Phil|    Hawkins|100715.76000000005|\n|  Heather|      Singh|105923.50999999995|\n|     Phil|      Singh|101363.09999999996|\n+---------+-----------+------------------+\n\nOut[6]: True"
     ]
    }
   ],
   "source": [
    "# Group by 'firstname' and 'lastname' and calculate the total revenue for each customer\n",
    "total_revenue_by_customer = df_trips.groupBy(\"firstname\", \"lastname\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
    "\n",
    "total_revenue_by_customer.show(10)\n",
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "total_revenue_by_customer_df = total_revenue_by_customer.toPandas()\n",
    "\n",
    "# Define the local path to save the file\n",
    "local_path = \"/tmp/total_revenue_by_customer.xlsx\"\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "total_revenue_by_customer_df.to_excel(local_path, index=False)\n",
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/total_revenue_by_customer.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f4b2a7-efe4-44bb-b224-430fcadeeb30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d) Show top 5 customer who paid maximum in total revenue by each Travel month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b92e2db-ccb6-4181-88fa-3fe3af9b21ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+------------------+----+\n|month|               name|     total_revenue|rank|\n+-----+-------------------+------------------+----+\n|    1|       Phil Hawkins|1777.3500000000001|   1|\n|    1|         Mary Singh|           1279.89|   2|\n|    1|         Phil Singh|           1246.78|   3|\n|    1|         Mary Singh|           1233.27|   4|\n|    1|       Phil Hawkins|           1178.33|   5|\n|    2|Russell Constantine|1154.6699999999998|   1|\n|    2|         Mary Singh|           1088.71|   2|\n|    2|       Phil Hawkins|           1079.17|   3|\n|    2|       Phil Hawkins| 994.3399999999999|   4|\n|    2|Russell Constantine|            983.87|   5|\n|    3|Russell Constantine|           1609.96|   1|\n|    3|      Heather Singh|           1078.05|   2|\n|    3|         Mary Singh|           1040.87|   3|\n|    3|      Robert Banner|            967.79|   4|\n|    3|Russell Constantine|            957.39|   5|\n|    4|         Mary Singh|           1192.19|   1|\n|    4|          Jay Singh|           1153.53|   2|\n|    4|      Robert Banner|           1114.45|   3|\n|    4|       Phil Hawkins|           1058.19|   4|\n|    4|        Billy Kohli|           1029.58|   5|\n+-----+-------------------+------------------+----+\nonly showing top 20 rows\n\nOut[16]: True"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, rank, month, concat, lit\n",
    "\n",
    "# Group by 'totalmonth', 'firstname', and 'lastname', then calculate total revenue for each customer per month\n",
    "revenue_by_customer_month = df_trips.groupBy(\"totalmonth\", \"firstname\", \"lastname\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
    "\n",
    "# Extract month from 'totalmonth' and create a new column 'month'\n",
    "revenue_by_customer_month = revenue_by_customer_month.withColumn(\"month\", month(col(\"totalmonth\")))\n",
    "\n",
    "# Combine firstname and lastname into a single column 'name'\n",
    "revenue_by_customer_month = revenue_by_customer_month.withColumn(\"name\", concat(col(\"firstname\"), lit(\" \"), col(\"lastname\")))\n",
    "\n",
    "# Define a window specification for ranking customers within each month based on total revenue\n",
    "window_spec = Window.partitionBy(\"month\").orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "# Add a rank column to rank customers within each month\n",
    "ranked_customers = revenue_by_customer_month.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Filter to get the top 5 customers by revenue for each month\n",
    "top_5_customers_per_month = ranked_customers.filter(col(\"rank\") <= 5)\n",
    "\n",
    "# Select the desired columns\n",
    "result_df = top_5_customers_per_month.select(\"month\", \"name\", \"total_revenue\", \"rank\")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "top_5_customers_per_month_df = result_df.toPandas()\n",
    "\n",
    "# Define the local path to save the file\n",
    "local_path = \"/tmp/top_5_customers_per_month.xlsx\"\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "top_5_customers_per_month_df.to_excel(local_path, index=False)\n",
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/top_5_customers_per_month.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8df9e7c-25fc-45d3-a62b-785fc4d25147",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "c) Total Flying Hours by TailNumber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a447fc0-fc5a-48c5-a97d-4d71931eb100",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n|tailnumber|total_flying_hours|\n+----------+------------------+\n|     T2335|               878|\n|     O2378|               838|\n|     C2386|               878|\n|     C2896|               888|\n|     H3059|               737|\n|     R2852|               750|\n|     C2750|               809|\n|     G2971|               871|\n|     O2349|               784|\n|     Y2161|               942|\n|     A2975|               827|\n|     N2252|               757|\n|     P2752|               808|\n|     Z2357|               874|\n|     F2838|               856|\n|     S2869|               765|\n|     J2465|               895|\n|     C2990|               863|\n|     T2366|               683|\n|     C2888|               792|\n+----------+------------------+\n\nOut[8]: True"
     ]
    }
   ],
   "source": [
    "# Group by 'tailnumber' and calculate the total flight time for each aircraft\n",
    "total_flying_hours_by_tailnumber = df_trips.groupBy(\"tailnumber\").agg(sum(\"totalflighttime\").alias(\"total_flying_hours\"))\n",
    "\n",
    "total_flying_hours_by_tailnumber.show()\n",
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "total_flying_hours_by_tailnumber_df = total_flying_hours_by_tailnumber.toPandas()\n",
    "\n",
    "# Define the local path to save the file\n",
    "local_path = \"/tmp/total_flying_hours_by_tailnumber.xlsx\"\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "total_flying_hours_by_tailnumber_df.to_excel(local_path, index=False)\n",
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/total_flying_hours_by_tailnumber.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8160793-235d-434b-a4c8-04cc158cf351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/FileStore/tables/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f79648-cf29-4452-9471-d657741e137c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "result_pd_df = aircrafts_df_with_make_final.toPandas()\n",
    "\n",
    "# Define the local path to save the file\n",
    "local_path = \"/tmp/trips_with_aircraft_make.xlsx\"\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "result_pd_df.to_excel(local_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa2f1ef-e92d-4bdc-9bc5-8d312e999d7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: True"
     ]
    }
   ],
   "source": [
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/trips_with_aircraft_make.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda30109-b79f-4072-a8a4-95a26488f500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the PySpark DataFrame to a Pandas DataFrame\n",
    "result_pdtrips_df = df_trips.toPandas()\n",
    "\n",
    "# Define the local path to save the file\n",
    "local_path = \"/tmp/trips.xlsx\"\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file in the local file system\n",
    "result_pdtrips_df.to_excel(local_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b63f36-6659-4fcd-8a19-95cc5efeffa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: True"
     ]
    }
   ],
   "source": [
    "# Define the DBFS path where the file will be saved\n",
    "dbfs_path = \"dbfs:/FileStore/tables/trips.xlsx\"\n",
    "\n",
    "# Copy the file from the local file system to DBFS\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29c937ac-1587-4301-89e0-2814634fe616",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "transform layer trips - Retain only Date from TravelDate field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0607eec2-ff4a-4703-896f-f847bdc60fcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'traveldate' column has been saved to 'travel_dates.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Select the 'traveldate' column from the DataFrame\n",
    "travel_date_df = df_trips.select(\"traveldate\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "travel_date_pd_df = travel_date_df.toPandas()\n",
    "\n",
    "# Import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file\n",
    "dbfs_path = \"dbfs:/FileStore/tables/travel_date.xlsx\"\n",
    "\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n",
    "print(\"The 'traveldate' column has been saved to 'travel_dates.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6f3cb0c-d58b-45b4-9107-c8fcd78c7c15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "agg layer aircraft - Find the total capacity by Make in current aircraft inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4bd67a0-5254-44bd-a41f-486d351b3996",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n|  Make|TotalCapacity|\n+------+-------------+\n|Boeing|       1997.0|\n|Airbus|        868.0|\n+------+-------------+\n\nThe 'traveldate' column has been saved to 'total_capacity_by_make.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Assuming the 'aircrafts_df_with_make' DataFrame already includes the 'Make' and 'Capacity' columns\n",
    "\n",
    "# Calculate the total capacity by 'Make'\n",
    "total_capacity_by_make = aircrafts_df_with_make.groupBy(\"Make\").agg(sum(\"Capacity\").alias(\"TotalCapacity\"))\n",
    "\n",
    "# Display the results\n",
    "total_capacity_by_make.show()\n",
    "# Convert to Pandas DataFrame\n",
    "total_capacity_by_make_df = total_capacity_by_make.toPandas()\n",
    "import pandas as pd\n",
    "\n",
    "# Write the Pandas DataFrame to an Excel file\n",
    "dbfs_path = \"dbfs:/FileStore/tables/total_capacity_by_make.xlsx\"\n",
    "\n",
    "dbutils.fs.cp(f\"file:{local_path}\", dbfs_path)\n",
    "print(\"The 'traveldate' column has been saved to 'total_capacity_by_make.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26911581-d2ef-4f33-b0bb-38ddb1de85c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "major project aircrafts+trips data csv",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
